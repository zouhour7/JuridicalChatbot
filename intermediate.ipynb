{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 78.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c10case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c11case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c12case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c13case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c14case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c15case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c16case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c17case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c18case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c19case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c1case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c20case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c2case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c3case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c4case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c5case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c6case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c7case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c8case.txt',\n",
       " 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c9case.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = DirectoryLoader('C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\', glob=\"*case.txt\",show_progress=True,loader_cls=TextLoader)\n",
    "data = loader.load()\n",
    "data\n",
    "doc_sources = [doc.metadata['source']  for doc in data]\n",
    "doc_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor, EmbeddingsFilter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "import torch\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the extracted data into text chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)\n",
    "text_chunks = text_splitter.split_documents(data)\n",
    "\n",
    "# Create embeddings for each text chunk\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "vector_store = FAISS.from_documents(text_chunks, embedding=embeddings).as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ChatOllama(model=\"mistral\",\n",
    "                  temperature=0,\n",
    "                  num_ctx=32768,\n",
    "                  top_k=10,\n",
    "                  top_p=0.5,\n",
    "                  keep_alive=-1,\n",
    "                  num_predict=4096,\n",
    "                  num_batch=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(model)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compressor = LLMChainExtractor.from_llm(model)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vector_store\n",
    ")\n",
    "x=compression_retriever.get_relevant_documents(\"who delivered the opinion of the court in the case of SILVERTHORNE LUMBER COMPANY, INC., ET AL. v. UNITED STATES. No. 358.?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='MR. JUSTICE HOLMES delivered the opinion of the court.\\nThis is a writ of error brought to reverse a judgment of the District Court fining the Silverthorne Lumber Company two hundred and fifty dollars for contempt of court and ordering Frederick W. Silverthorne to be imprisoned until he should purge himself of a similar contempt. The contempt in question was a refusal to obey subpoenas and an order of Court to produce books and documents of the company before the grand jury to be used in regard to alleged violation of the statutes of the United States by the said Silverthorne and his father. One ground of the refusal was that the order of the Court infringed the rights of the parties under the Fourth Amendment of the Constitution of the United States.', metadata={'source': 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c10case.txt'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "compressor = LLMChainExtractor.from_llm(model)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vector_store\n",
    ")\n",
    "x=compression_retriever.get_relevant_documents(\"who delivered the opinion of the court in the case of SILVERTHORNE LUMBER COMPANY, INC., ET AL. v. UNITED STATES. No. 358.?\")\n",
    "metadata = {'source': 'C:\\\\Users\\\\amine\\\\chatbot\\\\cases\\\\c10case.txt'}\n",
    "source = metadata['source']\n",
    "\n",
    "# Using regular expression to extract 'c' followed by a number\n",
    "match = re.search(r'c\\d+', source)\n",
    "case_identifier = match.group(0)\n",
    "print(case_identifier)\n",
    "# Construct path for the opinion file\n",
    "opinion_path = os.path.join(os.path.dirname(source), case_identifier + 'opinion.txt')\n",
    "\n",
    "# Read the opinion from the file\n",
    "with open(opinion_path, 'r') as opinion_file:\n",
    "    opinion = opinion_file.read()\n",
    "\n",
    "print(opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Opinion: The Supreme Court held that the Fourth Amendment protection against unlawful searches and seizures extends beyond just the physical possession of the seized items, but also prevents the government from using any knowledge gained through such an unlawful means. In this case, the Silverthorne Lumber Company and its officers refused to comply with subpoenas to produce documents after they had been illegally seized by government officials. The Court ruled that the government could not use the knowledge obtained from the illegal seizure, even if it later returned the originals of the seized materials. The Court reasoned that the essence of the Fourth Amendment is to forbid the acquisition of evidence in an unlawful way, and that this prohibition applies not only to the use of such evidence before a court but also to any subsequent use of the knowledge gained from it. The judgment was reversed, with two dissenting justices.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Construct path for the opinion file\n",
    "opinion_path = os.path.join(os.path.dirname(source), case_identifier + 'opinion.txt')\n",
    "\n",
    "# Read the opinion from the file\n",
    "with open(opinion_path, 'r') as opinion_file:\n",
    "    opinion = opinion_file.read()\n",
    "\n",
    "print(opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\amine\\anaconda3\\lib\\site-packages (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\amine\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\amine\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amine\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amine\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\amine\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af95066454f474188dfcdac7cdbb2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amine\\.cache\\huggingface\\hub\\models--yoru01--modelna. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270d306a7a784884bda6b5a68cda280d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"yoru01/modelna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76912be954554160abeb10119d5d549b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amine\\.cache\\huggingface\\hub\\models--distilbert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34c2ea5a07b47a9a8f73484c39e797a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668f70c484bc41488995ea5319ecbe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd494bfa2162474ea724ab57830475cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_tokenizer\\\\tokenizer_config.json',\n",
       " './my_tokenizer\\\\special_tokens_map.json',\n",
       " './my_tokenizer\\\\vocab.txt',\n",
       " './my_tokenizer\\\\added_tokens.json',\n",
       " './my_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./my_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is FIRSTNAME i am AGE AGE old and i work as JOBTYPE Engineer, i am from STATE i came to STATE to visit a friend, i entred in a forbidden land in San Diego and CITY CITY to know what possibly could be my punishement ?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import DistilBertForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = DistilBertForTokenClassification.from_pretrained('C:\\\\Users\\\\amine\\\\chatbot\\\\src\\\\my_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:\\\\Users\\\\amine\\\\chatbot\\\\src\\\\my_tokenizer\")\n",
    "\n",
    "# Load the label dictionary\n",
    "with open('C:\\\\Users\\\\amine\\\\chatbot\\\\data\\\\data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the input sentence\n",
    "sentence = input(\"Please enter a sentence: \")\n",
    "\n",
    "# Tokenize the sentence and get model predictions\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_token_ids = torch.argmax(logits, dim=-1).numpy().tolist()[0]\n",
    "id_to_label = {id: label for label, id in data.items()}\n",
    "predicted_labels = [id_to_label[id] for id in predicted_token_ids]\n",
    "\n",
    "# Remove the last two labels (corresponding to [SEP] and [PAD] tokens)\n",
    "predicted_labels = predicted_labels[:-2]\n",
    "\n",
    "# Merge consecutive labels of the same type\n",
    "merged_labels = []\n",
    "current_label = None\n",
    "for label in predicted_labels:\n",
    "    if label == 'O' or label != current_label:\n",
    "        merged_labels.append(label)\n",
    "        current_label = label\n",
    "    else:\n",
    "        merged_labels[-1] = label\n",
    "\n",
    "# Pair each word with its label\n",
    "output_words = []\n",
    "for word, label in zip(sentence.split(), merged_labels):\n",
    "    if label == 'O':\n",
    "        output_words.append(word)\n",
    "    else:\n",
    "        output_words.append(label[2:])\n",
    "\n",
    "# Join the output words into a sentence\n",
    "output_sentence = \" \".join(output_words)\n",
    "\n",
    "print(output_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load JSON file\n",
    "with open('C:\\\\Users\\\\amine\\\\output5.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create a directory to store text files\n",
    "if not os.path.exists('cases'):\n",
    "    os.makedirs('cases')\n",
    "\n",
    "# Iterate through cases\n",
    "for case_name, case_data in data['cases'].items():\n",
    "    case_content = case_data['case']\n",
    "    opinion_content = case_data['opinion']\n",
    "    \n",
    "    # Write case content to file\n",
    "    with open(f'cases/{case_name}case.txt', 'w') as case_file:\n",
    "        case_file.write(case_content)\n",
    "    \n",
    "    # Write opinion content to file\n",
    "    with open(f'cases/{case_name}opinion.txt', 'w') as opinion_file:\n",
    "        opinion_file.write(opinion_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the os module to work with file paths and directories\n",
    "import os\n",
    "\n",
    "# Defining a function to extract paragraphs from a text based on a specific delimiter\n",
    "def extract_paragraphs(text):\n",
    "    # Initializing an empty list to store the extracted paragraphs\n",
    "    paragraphs = []\n",
    "    # Initializing the starting index for searching for the delimiter\n",
    "    start_index = 0\n",
    "    # Looping indefinitely until the end of the text is reached\n",
    "    while True:\n",
    "        # Finding the starting index of the delimiter \"~~~~\" in the text\n",
    "        start_index = text.find(\"~~~~\", start_index)\n",
    "        # If the delimiter is not found, exit the loop\n",
    "        if start_index == -1:\n",
    "            break\n",
    "        # Finding the ending index of the delimiter \"~~~~\" in the text\n",
    "        end_index = text.find(\"~~~~\", start_index + 1)\n",
    "        # If the delimiter is not found, exit the loop\n",
    "        if end_index == -1:\n",
    "            break\n",
    "        # Extracting the paragraph between the delimiters and stripping whitespace\n",
    "        paragraph = text[start_index + len(\"~~~~\"):end_index].strip()\n",
    "        # Appending the extracted paragraph to the list of paragraphs\n",
    "        paragraphs.append(paragraph)\n",
    "        # Updating the start index for the next iteration\n",
    "        start_index = end_index + len(\"~~~~\")\n",
    "    # Returning the list of extracted paragraphs\n",
    "    return paragraphs\n",
    "\n",
    "# Defining a function to process a file, extract paragraphs, and write them to output files\n",
    "def process_file(file_path, output_directory):\n",
    "    # Opening the file in read mode\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Reading the entire contents of the file into a string variable\n",
    "        text = file.read()\n",
    "        # Extracting paragraphs from the text using the defined function\n",
    "        paragraphs = extract_paragraphs(text)\n",
    "        # Extracting the header from the text between the delimiters \"****\"\n",
    "        header = text[text.find(\"****\") + len(\"****\"):text.find(\"****\", text.find(\"****\") + 1)].strip()\n",
    "        # Iterating over each extracted paragraph\n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            # Generating the output file path based on the input file name and index\n",
    "            output_file_path = os.path.join(output_directory, f\"{os.path.splitext(os.path.basename(file_path))[0]}_{i}.txt\")\n",
    "            # Opening the output file in write mode\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                # Writing the header, paragraph, and a newline character to the output file\n",
    "                output_file.write(header + '\\n\\n' + paragraph)\n",
    "\n",
    "# Defining a function to process all files in a directory recursively\n",
    "def process_directory(input_directory, output_directory):\n",
    "    # Walking through the directory tree rooted at the input directory\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        # Iterating over each file found in the directory\n",
    "        for file in files:\n",
    "            # Checking if the file has a \".txt\" extension\n",
    "            if file.endswith(\".txt\"):\n",
    "                # Constructing the full file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Calling the process_file function to process the current file\n",
    "                process_file(file_path, output_directory)\n",
    "\n",
    "# Setting the input directory path where the text files are located\n",
    "input_directory_path = \"C:\\\\Users\\\\amine\\\\chatbot\\\\testfile\"\n",
    "# Setting the output directory path where the processed files will be written\n",
    "output_directory_path = \"C:\\\\Users\\\\amine\\\\chatbot\\\\testfile2\"\n",
    "\n",
    "# Calling the process_directory function to process all files in the input directory\n",
    "# and write the processed files to the output directory\n",
    "process_directory(input_directory_path, output_directory_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
